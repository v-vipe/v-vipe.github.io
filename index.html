<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="V-VIPE: Variational View Invariant Pose Embedding">
  <meta name="keywords" content="wayex, WayEx, waypoint-exploration">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>V-VIPE</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script type="text/javascript"
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">V-VIPE: Variational View Invariant Pose Embedding </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://mlevy2525.github.io/">Mara Levy</a>, </span>
            <span class="author-block">
              <a href="http://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of Maryland, College Park</span>
            <!-- <span class="author-block"><sup>2</sup>Google Research</span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="index.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span> 
                  </a>
                  -->
            </div>
          </div>
           <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
                  <span class="link-block">
                    <a href="paper.pdf"
                       class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>

          </div>
           <!-- <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <div class="publication-video">
                <video id="teaser" autoplay muted loop playsinline height="100%" controls>
                    <source src="./static/videos/trailer_dataset1_final.mp4"
                            type="video/mp4">
                  </video>
              </div>

            </div> </div>-->

<!--     <section class="section">
    <!--/ Abstract. -->
    
<!-- 
              <div class="container is-max-desktop">
                <h2 class="title is-3">Project Video</h2>
                <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/WayEx_HighQuality.mp4"
                      type="video/mp4">
            </video>
                </a>
              </div> -->
          
          <!-- <div class="subtitle has-text-centered">
      <h2 class="title is-4">
        Video coming soon
      </h2> </div> -->
        </div>

      </div>
    </div>
  </div>
</section> -->
<hr>
  <section class="section">
    <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
Learning to represent three dimensional (3D) human pose given a two dimensional (2D) image of a person, is a challenging problem. In order to make the problem less ambiguous it has become common practice to estimate 3D pose in the camera coordinate space. However, this makes the task of comparing two 3D poses difficult. In this paper, we address this challenge by separating the problem of estimating 3D pose from 2D images into two steps. We use a variational autoencoder (VAE) to find an embedding that represents 3D poses in canonical coordinate space. We refer to this embedding as variational view-invariant pose embedding (V-VIPE). Using V-VIPE we can encode 2D and 3D poses and use the embedding for downstream tasks, like retrieval and classification. We can estimate 3D poses from these embeddings using the decoder as well as generate unseen 3D poses. The variability of our encoding allows it to generalize well to unseen camera views when mapping from 2D space. To the best of our knowledge, V-VIPE is the only representation to offer this diversity of applications.          </p>
        </div>
      </div>
    </div>
    <div class="columns is-centered">
        <img src="./static/images/vvipe_teaser.jpg"/>
    </div>
    </section>

<hr>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Data Processing</h2> 
    <div class="content has-text-justified">
          <p>
            In typical 3D pose estimation work the pose is found in camera space. However as, seen in the image below a pose taken from different viewpoints can be very different, even when represented in 3D.
          </p>

          <div class="columns is-centered">
            <img src="./static/images/many_camera.jpg"/>
          </div>

          <p>In order to compare poses from different view points we remove any rotation from the original pose and train only on 3D poses that are aligned to each plane.</p>

          <div class="columns is-centered">
            <img src="./static/images/rotated.jpg"/>
          </div>
    </div>
  </div>
</section>

<hr>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Model Overview</h2> 
    <div class="content has-text-justified">
          <p>
            V-Vipe has 3 components. A data processing step in which any rotation information is removed fro the 3D representation as described above. 
            Next a 3D Encoder and Decoder are trained as a Variational Auto Encoder. In addition to the VAE losses a triplet loss is added which pushes similar poses close together in the embedding space.
            Finally a 2D Encoder is trained to map 2D poses to the embedding space defined above.
          </p>
    </div>

    <div class="columns is-centered">
      <img src="./static/images/full_network.jpg"/>
    </div>
  </div>
</section>

<hr>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Pose Retrieval</h2> 
    <div class="content has-text-justified">
          <p>
            Using the 2D Embedder described above we can find the embeddings for many 2D poses from different camera view points and query any pose to find the most similar pose from another view point.
            Following prior work we evaluate how often our model finds a similar pose by looking at the hit metric. Because our model has generative capabilities we find that it generalizes better to unseen poses as well as unseen camera viewpoints.
          </p>
    </div>

    <div class="columns is-centered">
      <img src="./static/images/retrieved.jpg"/>
    </div>

    <div class="columns is-centered">
      <img src="./static/images/retrieval_table.png"/>
    </div>
  </div>
</section>

<hr>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Pose Estimation</h2> 
    <div class="content has-text-justified">
          <p>
            In addition to being able to retrieve similar poses our method can estimate new poses.
          </p>
    </div>

    <div class="columns is-centered">
      <img src="./static/images/pose_estimation.jpg"/>
    </div>
  </div>
</section>

<hr>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Pose Generation</h2> 
    <div class="content has-text-justified">
          <p>
            We can add noise to the embedding space to find new poses
          </p>
    </div>

    <div class="columns is-centered">
      <img src="./static/images/new poses.jpg"/>
    </div>

    <div class="content has-text-justified">
      <p>
        Additionally, we can take two poses and generate poses in between.
      </p>
    </div>

    <div class="columns is-centered">
      <img src="./static/images/between poses.jpg"/>
    </div>
  </div>
</section>

<!--   <hr> -->
<!-- <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Proximal Reward Function</h2> 
    <div class="content has-text-justified">
          <p>
            Once we have determined if two points are proximal we can use this information to find the reward for each state during training. 
            We model this reward off a sparse reward. If we assume that if the reward is 0 at the last frame of the expert trajectory and -1 at all other frames we can compute the pseudo ground truth reward for each frame and apply this reward to all proximal waypoints.
            If the two points are proximal and the waypoint is at step t in an episode of length l the reward is as follows.
          </p>

    </div>

    <div class="columns is-centered">
      <img src="./static/images/proximal_reward.png"/>
    </div>

    <p>
      If the two points are not proximal and l max represents the maximum possible length of a demonstration than the reward is as follows.
    </p>

    <div class="columns is-centered">
      <img src="./static/images/not_proximal_reward.png"/>
    </div>
  </div>

</section> -->

<!--   <hr>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Expansion</h2> 
    <div class="content has-text-justified">
          <p>
            When the first part of training is done the policy will have learned how to trace the expert trajectory.
          </p>
    </div>

    <div class="columns is-centered">
      <img src="./static/images/traced_policy.jpg"/>
    </div>

    <div class="content has-text-justified">
          <p>
            From here we want to expand the policy to cover a larger range of start and end states. We do this by applying increasing amounts of noise to the start and goal state so long as the success rate stays high.
          </p>
    </div>

    <div class="columns is-centered">
      <img src="./static/images/small_expanded_policy.jpg"/>
    </div>

    <div class="content has-text-justified">
          <p>
            While the first expansion is small eventually we will be able to cover the full range of start and goal states.
          </p>
    </div>

    <div class="columns is-centered">
      <img src="./static/images/large_expanded_policy.jpg"/>
    </div>




</section> -->
  
<!-- <hr>
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Results</h2>
    <div class="content has-text-justified">
      </div>

     <div class="columns is-centered">
      <div class="column is-full-width">
        <img src="./static/images/lift_web.jpg"/>
      </div>
    </div>
    
    <div class="columns is-centered">
      <div class="column is-full-width">
        <img src="./static/images/door_rs_web.jpg"/>
      </div>
    </div>
    
    <div class="columns is-centered">
      <div class="column is-full-width">

        <img src="./static/images/door_web.jpg"/>
        
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <img src="./static/images/insertion_web.jpg"/>
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <img src="./static/images/assembly_web.jpg"/>
      </div>
    </div>
  </div>
</section> -->

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
          <p>
            Credits for this awesome template goes to <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
